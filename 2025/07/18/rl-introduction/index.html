<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.23.2","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="强化学习基础介绍 ps. 本来不是研究强化学习这方面的知识，但奈何强化学习已经在各顶会中崭露头角，本科和硕士期间都在学校简单接触过，但是也是学完就扔了，因此打算写成博客来系统性学习记录一下。 本系列主要参考：  西湖大学赵世钰老师《强化学习的数学原理》 OpenAI 官方强化学习课程 UCLA RL-LLM 2025 Spring  下面就开始进入强化学习的世界吧 ！！！ 1. 强化学习重要性 以">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning Introduction">
<meta property="og:url" content="http://example.com/2025/07/18/rl-introduction/index.html">
<meta property="og:site_name" content="Liam62&#39;s Blog">
<meta property="og:description" content="强化学习基础介绍 ps. 本来不是研究强化学习这方面的知识，但奈何强化学习已经在各顶会中崭露头角，本科和硕士期间都在学校简单接触过，但是也是学完就扔了，因此打算写成博客来系统性学习记录一下。 本系列主要参考：  西湖大学赵世钰老师《强化学习的数学原理》 OpenAI 官方强化学习课程 UCLA RL-LLM 2025 Spring  下面就开始进入强化学习的世界吧 ！！！ 1. 强化学习重要性 以">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2025/07/18/rl-introduction/image-20250718024647167.png">
<meta property="og:image" content="http://example.com/2025/07/18/rl-introduction/image-20250718024830657.png">
<meta property="og:image" content="http://example.com/2025/07/18/rl-introduction/image-20250728011535412.png">
<meta property="og:image" content="http://example.com/2025/07/18/rl-introduction/image-20250729230638869.png">
<meta property="og:image" content="http://example.com/2025/07/18/rl-introduction/image-20250728011620617.png">
<meta property="og:image" content="http://example.com/2025/07/18/rl-introduction/image-20250729230610733.png">
<meta property="og:image" content="http://example.com/2025/07/18/rl-introduction/image-20250728012314561.png">
<meta property="article:published_time" content="2025-07-17T17:54:23.000Z">
<meta property="article:modified_time" content="2025-07-30T11:07:23.446Z">
<meta property="article:author" content="Liam">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2025/07/18/rl-introduction/image-20250718024647167.png">


<link rel="canonical" href="http://example.com/2025/07/18/rl-introduction/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2025/07/18/rl-introduction/","path":"2025/07/18/rl-introduction/","title":"Reinforcement Learning Introduction"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Reinforcement Learning Introduction | Liam62's Blog</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Liam62's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">强化学习基础介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%87%8D%E8%A6%81%E6%80%A7"><span class="nav-number">1.1.</span> <span class="nav-text">1. 强化学习重要性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.</span> <span class="nav-text">2. 什么是强化学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#markov-decision-processes-mdp"><span class="nav-number">1.3.</span> <span class="nav-text">3. Markov Decision Processes,
MDP</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Liam</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/18/rl-introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Liam">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liam62's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Reinforcement Learning Introduction | Liam62's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Reinforcement Learning Introduction
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-18 01:54:23" itemprop="dateCreated datePublished" datetime="2025-07-18T01:54:23+08:00">2025-07-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-30 19:07:23" itemprop="dateModified" datetime="2025-07-30T19:07:23+08:00">2025-07-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="强化学习基础介绍">强化学习基础介绍</h1>
<p>ps.
本来不是研究强化学习这方面的知识，但奈何强化学习已经在各顶会中崭露头角，本科和硕士期间都在学校简单接触过，但是也是学完就扔了，因此打算写成博客来系统性学习记录一下。</p>
<p>本系列主要参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning">西湖大学赵世钰老师《强化学习的数学原理》</a></li>
<li><a target="_blank" rel="noopener" href="https://spinningup.openai.com/en/latest/">OpenAI
官方强化学习课程</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=q9972BRoXzQ">UCLA RL-LLM
2025 Spring</a></li>
</ul>
<p>下面就开始进入强化学习的世界吧 ！！！</p>
<h2 id="强化学习重要性">1. 强化学习重要性</h2>
<p>以前接触 RL 基本上只是在很经典很老的一些算法层面，比如 MC Learning,
TD Learning, Q-Learning 这种，但是最近 RL 在 LLM
中表现过于亮眼，因此很有必要学习更深入的 RL
方法。此外，与传统的监督学习相比（只是在模仿答案），RL
可以算得上是真正的具有智慧的机器学习方法。因此首先举出 RL 在 LLM
上的优点：</p>
<ul>
<li>通过 RL，大语言模型 (LLM)可以学会如何使用思维链
(CoT)，这大致意味着在回答问题前先自我对话。人类内部的思考过程导致的文本，在预训练数据中大多是缺失的，但我们可以用
RL 来复现这种能力。Deepseek R1 和 OpenAI o1 (o3)
已经明确展示了这一点。对这种方法的进一步优化肯定会带来更多进步。</li>
<li>强化学习可能是发现正确因果关系、消除启发式方法中误解的关键，而不是仅仅依赖表面经验规则。举个例子，打篮球的时候看运动员的视频学技术，但如果只照搬表面步骤，往往会失败。要学会这一点，仅靠观察是不够的，你必须亲自尝试这个动作，当你用对的方法成功突破并得分时，这种“被奖励”的体验会强化你的记忆，让你真正学会。</li>
</ul>
<h2 id="什么是强化学习">2. 什么是强化学习</h2>
<ul>
<li><strong>一句话概括</strong>：智能体 (agent) 根据环境 (environment)
反馈的奖励 (reward），调整自己的行为策略
(policy)，以实现长期目标的最大化。</li>
<li><strong>简单举例</strong>：训练一只狗学会坐下。每当狗狗坐下时，你会给它奖励（比如小零食）；如果它没有按照指令做，你不给奖励（甚至轻微惩罚）。狗狗通过多次尝试，逐渐学会了“坐下”这个动作能带来奖励，所以以后更愿意这样做。</li>
<li><strong>强化学习 vs 监督学习</strong>：
<ul>
<li><strong>监督学习</strong>：<strong>Input-Output Pairs</strong>
数据格式，<strong>学习两者之间的映射关系</strong>；监督学习是单一选择问题，如分类。</li>
<li><strong>强化学习</strong>：数据是智能体和环境交互过程中“生成”的，这些数据通常被称为
<strong>“经验 (Experience)”</strong> 或 <strong>“轨迹
(Trajectory)”</strong>，<strong>学习的是一个策略函数</strong>，即在不同状态下如何选择动作，以最大化累计奖励；强化学习是决策序列问题（多个选择）。</li>
</ul></li>
</ul>
<h2 id="markov-decision-processes-mdp">3. Markov Decision Processes,
MDP</h2>
<p>在正式介绍 RL 之前，需要先了解清楚其理论基础马尔可夫决策过程
(MDP)。RL 和 MDP 之间的关系可以看作：<strong>MDP 是问题的“建模框架”，RL
是“求解方法”</strong>。</p>
<ul>
<li><strong>定义</strong>：A sequential decision problem for a fully
observable, stochastic environment with a Markovian transition model and
additive rewards is called a Markov decision process，即首先 MDP 是一个
<strong>序列决策 (Sequential Decision)</strong> 问题，其次是
<strong>完全可观测</strong>、<strong>具有马尔可夫转移模型</strong>，并且
<strong>伴有奖励/惩罚机制</strong>。</li>
</ul>
<p>​
<img src="/2025/07/18/rl-introduction/image-20250718024647167.png" alt="image-20250718024647167" style="zoom:50%;"><img src="/2025/07/18/rl-introduction/image-20250718024830657.png" alt="image-20250718024830657" style="zoom:50%;"></p>
<p>RL / MDP 通常由以下几个核心要素组成：</p>
<ul>
<li><strong>智能体 (Agent)</strong>：学习和做决策的主体。</li>
<li><strong>环境
(Environment)</strong>：智能体进行交互的对象。当智能体采取动作后，环境会反馈新的状态和奖励。</li>
<li><strong>状态 (State,
s)</strong>：环境在某个时刻的具体情况。比如，棋盘的当前布局、迷宫中当前位置等；也可能是一个图像、一个向量、或一组离散变量。</li>
<li><strong>动作 (Action,
a)</strong>：智能体在某一状态下可以采取的行为选择。比如向上或向右移动。</li>
<li><strong>策略 (Policy,
π)</strong>：智能体在每个状态下选择动作的规则或概率分布，即状态到动作的映射（输入状态，输出动作）。可以是确定性
<span class="math inline"><em>a</em><sub><em>t</em></sub> = <em>μ</em>(<em>s</em><sub><em>t</em></sub>)</span>
的或随机的 <span class="math inline"><em>a</em><sub><em>t</em></sub> ∼ <em>π</em>(·|<em>s</em><sub><em>t</em></sub>)</span>，比如在当前状态下的策略为向上移动，或
50% 向上 50% 向右。</li>
<li><strong>转移模型 (Transition model,
T)</strong>：描述当智能体在某个状态 <span class="math inline"><em>s</em></span> 下采取动作 <span class="math inline"><em>a</em></span> 后，会以什么概率转移到下一个状态
<span class="math inline"><em>s</em><sup>′</sup></span>，这个概率可以使用
<span class="math inline"><em>P</em>(<em>s</em><sup>′</sup> ∣ <em>s</em>, <em>a</em>)</span>
表示。可以是确定性的或随机的，比如策略决定向右移动后，实际下一个状态可能是
80% 在原位置的右边（向右），20%
在原位置下面（向下）。<strong><u>这里的随机性是环境所决定的。</u></strong></li>
<li><strong>奖励 (Reward,
r)</strong>：环境对智能体采取某个（单个）动作后的
<strong><u>即时</u></strong>
反馈。取决于当前状态、刚刚采取的行动以及下一个状态。即 <span class="math inline"><em>r</em><sub><em>t</em></sub> = <em>R</em>(<em>s</em><sub><em>t</em></sub>, <em>a</em><sub><em>t</em></sub>, <em>s</em><sub><em>t</em> + 1</sub>)</span>。</li>
<li><strong>回报 (Return,
R/G)</strong>：从当前时刻开始，未来能获得的所有奖励的
<strong><u>累计</u></strong> 和。</li>
</ul>
<hr>
<ul>
<li><p><strong>价值函数 (Value
Function)</strong>：衡量某个状态（或状态-动作对）长期回报好坏的函数。常见的有状态价值函数
<span class="math inline"><em>V</em>(<em>s</em>)</span> 和动作价值函数
<span class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>)</span>：</p>
<ul>
<li><p><strong>状态价值函数</strong> <span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span>：在固定策略
<span class="math inline"><em>π</em></span> 下，从状态 <span class="math inline"><em>s</em></span>
出发以后，智能体期望获得的累计奖励。</p>
<p><img src="/2025/07/18/rl-introduction/image-20250728011535412.png" alt="image-20250728011535412" style="zoom:40%;"></p>
<ul>
<li><p><strong>最优状态价值函数</strong> <span class="math inline"><em>V</em><sup>*</sup>(<em>s</em>)</span>：在 <span class="math inline"><em>s</em></span> 下采取 <strong>最优策略</strong>
所能获得的最大期望累计回报。 <span class="math display"><em>V</em><sup>*</sup>(<em>s</em>) = max<sub><em>π</em></sub><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span></p></li>
<li><p><strong>转移性质：</strong></p>
<p><img src="/2025/07/18/rl-introduction/image-20250729230638869.png" alt="image-20250729230638869" style="zoom:40%;"></p></li>
</ul></li>
<li><p><strong>动作价值函数</strong> <span class="math inline"><em>Q</em><sup><em>π</em></sup>(<em>s</em>, <em>a</em>)</span>：在固定策略
<span class="math inline"><em>π</em></span> 下，智能体在状态 <span class="math inline"><em>s</em></span> 采取特定动作 <span class="math inline"><em>a</em></span> 后，再继续按照策略 <span class="math inline"><em>π</em></span> 行动时所能获得的累计奖励。</p>
<p><img src="/2025/07/18/rl-introduction/image-20250728011620617.png" alt="image-20250728011620617" style="zoom:40%;"></p>
<ul>
<li><p><strong>最优动作价值函数</strong> <span class="math inline"><em>Q</em><sup>*</sup>(<em>s</em>, <em>a</em>)</span>：在
<span class="math inline"><em>s</em></span> 采取 <span class="math inline"><em>a</em></span>，然后 <strong>之后都最优</strong>
地行动所能获得的最大期望累计回报。 <span class="math display"><em>Q</em><sup>*</sup>(<em>s</em>, <em>a</em>) = max<sub><em>π</em></sub><em>Q</em><sup><em>π</em></sup>(<em>s</em>, <em>a</em>)</span></p></li>
<li><p><strong>转移性质：</strong></p>
<p><img src="/2025/07/18/rl-introduction/image-20250729230610733.png" alt="image-20250729230610733" style="zoom:40%;"></p></li>
</ul></li>
<li><p><span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span>
和 <span class="math inline"><em>Q</em><sup><em>π</em></sup>(<em>s</em>, <em>a</em>)</span>
关系：<strong>加权期望</strong>。</p>
<p><img src="/2025/07/18/rl-introduction/image-20250728012314561.png" alt="image-20250728012314561" style="zoom:40%;"></p></li>
<li><p><span class="math inline"><em>V</em><sup>*</sup>(<em>s</em>)</span> 和 <span class="math inline"><em>Q</em><sup>*</sup>(<em>s</em>, <em>a</em>)</span>
之间的关系： <span class="math display"><em>V</em><sup>*</sup>(<em>s</em>) = max<sub><em>a</em></sub><em>Q</em><sup>*</sup>(<em>s</em>, <em>a</em>)</span></p>
<p><span class="math display"><em>Q</em><sup>*</sup>(<em>s</em>, <em>a</em>) = 𝔼<sub><em>s</em><sup>′</sup>, <em>r</em></sub>[<em>r</em> + <em>γ</em><em>V</em><sup>*</sup>(<em>s</em><sup>′</sup>)]</span></p></li>
</ul></li>
</ul>
<hr>
<ul>
<li><p><strong>贝尔曼方程 (Bellman
Equation)</strong>：揭示两个状态之间的关系</p>
<ul>
<li><p><span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span>：在
<span class="math inline"><em>s</em></span> 按 <span class="math inline"><em>π</em></span> 选择动作 <span class="math inline"><em>a</em></span>，到 <span class="math inline"><em>s</em><sup>′</sup></span> 并获得奖励 <span class="math inline"><em>r</em></span>，未来递归。 <span class="math display"><em>V</em><sup><em>π</em></sup>(<em>s</em>) = 𝔼<sub><em>a</em> ∼ <em>π</em>(⋅|<em>s</em>), (<em>r</em>, <em>s</em><sup>′</sup>) ∼ <em>p</em>(⋅, ⋅|<em>s</em>, <em>a</em>)</sub>[<em>r</em> + <em>γ</em><em>V</em><sup><em>π</em></sup>(<em>s</em><sup>′</sup>)]</span></p>
<p><span class="math display"><em>V</em><sup><em>π</em></sup>(<em>s</em>) = ∑<sub><em>a</em></sub><em>π</em>(<em>a</em>|<em>s</em>)∑<sub><em>s</em><sup>′</sup></sub><em>p</em>(<em>s</em><sup>′</sup>|<em>s</em>, <em>a</em>)[<em>r</em>(<em>s</em>, <em>a</em>, <em>s</em><sup>′</sup>) + <em>γ</em><em>V</em><sup><em>π</em></sup>(<em>s</em><sup>′</sup>)]</span></p></li>
<li><p><span class="math inline"><em>Q</em><sup><em>π</em></sup>(<em>s</em>, <em>a</em>)</span>：在
<span class="math inline"><em>s</em></span> 做 <span class="math inline"><em>a</em></span>，到 <span class="math inline"><em>s</em><sup>′</sup></span>，再按 <span class="math inline"><em>π</em></span> 选择 <span class="math inline"><em>a</em><sup>′</sup></span> 递归。 <span class="math display"><em>Q</em><sup><em>π</em></sup>(<em>s</em>, <em>a</em>) = 𝔼<sub>(<em>r</em>, <em>s</em><sup>′</sup>) ∼ <em>p</em>(⋅, ⋅|<em>s</em>, <em>a</em>), <em>a</em><sup>′</sup> ∼ <em>π</em>(⋅|<em>s</em><sup>′</sup>)</sub>[ <em>r</em> + <em>γ</em><em>Q</em><sup><em>π</em></sup>(<em>s</em><sup>′</sup>, <em>a</em><sup>′</sup>) ]</span></p>
<p><span class="math display"><em>Q</em><sup><em>π</em></sup>(<em>s</em>, <em>a</em>) = ∑<sub><em>s</em><sup>′</sup></sub><em>p</em>(<em>s</em><sup>′</sup>|<em>s</em>, <em>a</em>)[<em>r</em>(<em>s</em>, <em>a</em>, <em>s</em><sup>′</sup>) + <em>γ</em>∑<sub><em>a</em><sup>′</sup></sub><em>π</em>(<em>a</em><sup>′</sup>|<em>s</em><sup>′</sup>)<em>Q</em><sup><em>π</em></sup>(<em>s</em><sup>′</sup>, <em>a</em><sup>′</sup>)]</span></p></li>
<li><p><span class="math inline"><em>V</em><sup>*</sup>(<em>s</em>)</span>。 <span class="math display"><em>V</em><sup>*</sup>(<em>s</em>) = max<sub><em>a</em></sub> 𝔼<sub>(<em>r</em>, <em>s</em><sup>′</sup>) ∼ <em>p</em>(⋅, ⋅|<em>s</em>, <em>a</em>)</sub>[ <em>r</em> + <em>γ</em><em>V</em><sup>*</sup>(<em>s</em><sup>′</sup>) ]</span></p>
<p><span class="math display"><em>V</em><sup>*</sup>(<em>s</em>) = max<sub><em>a</em></sub>∑<sub><em>s</em><sup>′</sup></sub><em>p</em>(<em>s</em><sup>′</sup>|<em>s</em>, <em>a</em>)[<em>r</em>(<em>s</em>, <em>a</em>, <em>s</em><sup>′</sup>) + <em>γ</em><em>V</em><sup>*</sup>(<em>s</em><sup>′</sup>)]</span></p></li>
<li><p><span class="math inline"><em>Q</em><sup>*</sup>(<em>s</em>, <em>a</em>)</span>。
<span class="math display"><em>Q</em><sup>*</sup>(<em>s</em>, <em>a</em>) = 𝔼<sub>(<em>r</em>, <em>s</em><sup>′</sup>) ∼ <em>p</em>(⋅, ⋅|<em>s</em>, <em>a</em>)</sub>[ <em>r</em> + <em>γ</em>max<sub><em>a</em><sup>′</sup></sub><em>Q</em><sup>*</sup>(<em>s</em><sup>′</sup>, <em>a</em><sup>′</sup>) ]</span></p>
<p><span class="math display"><em>Q</em><sup>*</sup>(<em>s</em>, <em>a</em>) = ∑<sub><em>s</em><sup>′</sup></sub><em>p</em>(<em>s</em><sup>′</sup>|<em>s</em>, <em>a</em>)[<em>r</em>(<em>s</em>, <em>a</em>, <em>s</em><sup>′</sup>) + <em>γ</em>max<sub><em>a</em><sup>′</sup></sub><em>Q</em><sup>*</sup>(<em>s</em><sup>′</sup>, <em>a</em><sup>′</sup>)]</span></p></li>
</ul></li>
</ul>
<hr>
<p>这里对奖励 (Reward)，回报
(Return)，价值函数之间的关系做个详细解释：</p>
<table>
<colgroup>
<col style="width: 9%">
<col style="width: 23%">
<col style="width: 35%">
<col style="width: 31%">
</colgroup>
<thead>
<tr>
<th>项目</th>
<th>奖励 Reward (<span class="math inline"><em>r</em></span>)</th>
<th>回报 Return (<span class="math inline"><em>G</em><sub><em>t</em></sub></span>)</th>
<th>价值函数 Value Function (<span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span>,
<span class="math inline"><em>Q</em><sup><em>π</em></sup>(<em>s</em>, <em>a</em>)</span>)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>定义</strong></td>
<td>智能体与环境交互时，每一步获得的即时反馈</td>
<td>从当前时刻起，未来所有奖励的累计和（通常带折扣）</td>
<td>从某状态（或状态-动作对）出发，按策略未来回报的期望值</td>
</tr>
<tr>
<td><strong>数学表达</strong></td>
<td><span class="math inline"><em>r</em><sub><em>t</em> + 1</sub></span></td>
<td><span class="math inline"><em>G</em><sub><em>t</em></sub> = <em>r</em><sub><em>t</em> + 1</sub> + <em>γ</em><em>r</em><sub><em>t</em> + 2</sub> + <em>γ</em><sup>2</sup><em>r</em><sub><em>t</em> + 3</sub> + ⋯</span></td>
<td><span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>) = 𝔼<sub><em>π</em></sub>[<em>G</em><sub><em>t</em></sub> ∣ <em>s</em><sub><em>t</em></sub> = <em>s</em>]</span></td>
</tr>
<tr>
<td><strong>作用范围</strong></td>
<td>只影响当前这一步</td>
<td>覆盖从当前到未来所有步骤</td>
<td>评价一个状态或状态-动作对的“长期好坏”</td>
</tr>
<tr>
<td><strong>依赖对象</strong></td>
<td>当前状态和动作</td>
<td>当前及未来所有奖励</td>
<td>策略、状态（和动作）以及所有可能的未来轨迹</td>
</tr>
<tr>
<td><strong>是否有随机性</strong></td>
<td>单步直接反馈，随环境变化</td>
<td>每次经历后都能算出一个具体值，可能不同</td>
<td>多次经历的平均结果（期望），理论上不随采样波动</td>
</tr>
<tr>
<td><strong>举例</strong></td>
<td>当前位置走一步，得到 <span class="math inline">−1</span></td>
<td>迷宫从起点出发本轮累计得分 <span class="math inline"><em>G</em><sub>0</sub> = 81</span></td>
<td>起点采用策略 <span class="math inline"><em>π</em></span>
平均每次能拿 <span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em><sub>0</sub>) = 82</span></td>
</tr>
<tr>
<td><strong>强化学习目标</strong></td>
<td>不是最终优化目标，只是反馈信号</td>
<td>不是直接优化目标，是中间变量</td>
<td><strong>最终优化目标：最大化价值函数（即最大化期望回报）</strong></td>
</tr>
</tbody>
</table>
<p><strong>总结：奖励是单步反馈，回报是一条轨迹的累计奖励，价值函数是回报的期望值，也是强化学习要优化的目标。</strong></p>
<ul>
<li><p><strong>MDP 数学形式</strong>： <span class="math display">⟨<em>S</em>, <em>A</em>, <em>R</em>, <em>P</em>, <em>ρ</em><sub>0</sub>⟩</span></p>
<ul>
<li><strong>S</strong> 表示所有有效状态的集合</li>
<li><strong>A</strong> 表示所有有效动作的集合</li>
<li><strong>R: S×A×S→R</strong> 是奖励函数，其定义为 <span class="math inline"><em>r</em><sub><em>t</em></sub> = <em>R</em>(<em>s</em><sub><em>t</em></sub>, <em>a</em><sub><em>t</em></sub>, <em>s</em><sub><em>t</em> + 1</sub>)</span></li>
<li><strong>P: S×A→P(S)</strong> 是状态转移概率函数，其中 <span class="math inline"><em>P</em>(<em>s</em><sup>′</sup> ∣ <em>s</em>, <em>a</em>)</span>
表示如果你处于状态 <span class="math inline"><em>s</em></span>
并采取动作 <span class="math inline"><em>a</em></span>，则转移到状态
<span class="math inline"><em>s</em><sup>′</sup></span>的概率</li>
<li><strong>ρ0</strong> 是初始状态分布。</li>
</ul></li>
<li><p><strong>轨迹概率分布：</strong>轨迹用 <span class="math inline"><em>τ</em></span> 表示，<span class="math inline"><em>τ</em> = (<em>s</em><sub>0</sub>, <em>a</em><sub>0</sub>, <em>s</em><sub>1</sub>, <em>a</em><sub>1</sub>, ...)</span>。假设转移函数和策略都是随机的，此时
<span class="math inline"><em>T</em></span> 步轨迹的概率为 ： <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.819ex;" xmlns="http://www.w3.org/2000/svg" width="39.677ex" height="6.73ex" role="img" focusable="false" viewbox="0 -1728.7 17537.2 2974.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"/></g><g data-mml-node="mo" transform="translate(1657,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(1935,0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"/></g><g data-mml-node="mo" transform="translate(2505,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(3171.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="msub" transform="translate(4227.6,0)"><g data-mml-node="mi"><path data-c="1D70C" d="M58 -216Q25 -216 23 -186Q23 -176 73 26T127 234Q143 289 182 341Q252 427 341 441Q343 441 349 441T359 442Q432 442 471 394T510 276Q510 219 486 165T425 74T345 13T266 -10H255H248Q197 -10 165 35L160 41L133 -71Q108 -168 104 -181T92 -202Q76 -216 58 -216ZM424 322Q424 359 407 382T357 405Q322 405 287 376T231 300Q217 269 193 170L176 102Q193 26 260 26Q298 26 334 62Q367 92 389 158T418 266T424 322Z"/></g><g data-mml-node="mn" transform="translate(550,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/></g></g><g data-mml-node="mo" transform="translate(5181.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(5570.1,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mn" transform="translate(502,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/></g></g><g data-mml-node="mo" transform="translate(6475.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="munderover" transform="translate(7031.3,0)"><g data-mml-node="mo" transform="translate(61.7,0)"><path data-c="220F" d="M220 812Q220 813 218 819T214 829T208 840T199 853T185 866T166 878T140 887T107 893T66 896H56V950H1221V896H1211Q1080 896 1058 812V-311Q1076 -396 1211 -396H1221V-450H725V-396H735Q864 -396 888 -314Q889 -312 889 -311V896H388V292L389 -311Q405 -396 542 -396H552V-450H56V-396H66Q195 -396 219 -314Q220 -312 220 -311V812Z"/></g><g data-mml-node="TeXAtom" transform="translate(121.3,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(1139,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(0,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"/></g><g data-mml-node="mo" transform="translate(704,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1482,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mi" transform="translate(8599.5,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(9350.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(9739.5,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="TeXAtom" transform="translate(502,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mn" transform="translate(1139,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mo" transform="translate(11450.4,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msub" transform="translate(11728.4,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g></g><g data-mml-node="mo" transform="translate(12535.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(12980.4,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g></g><g data-mml-node="mo" transform="translate(13847.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mi" transform="translate(14236.6,0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"/></g><g data-mml-node="mo" transform="translate(14806.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(15195.6,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g></g><g data-mml-node="mo" transform="translate(16062.9,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msub" transform="translate(16340.9,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g></g><g data-mml-node="mo" transform="translate(17148.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span> 其中， <span class="math inline"><em>ρ</em><sub>0</sub>(<em>s</em><sub>0</sub>)</span>
为初始状态 <span class="math inline"><em>s</em><sub>0</sub></span>
的分布。预期回报 <span class="math inline"><em>J</em>(<em>π</em>)</span>
为： <span class="math display"><em>J</em>(<em>π</em>) = ∫<sub><em>τ</em></sub><em>P</em>(<em>τ</em>|<em>π</em>)<em>R</em>(<em>τ</em>) = <em>E</em><sub><em>τ</em> ∼ <em>π</em></sub><em>R</em>(<em>τ</em>)</span>
<strong>因此，强化学习中的核心优化问题可以表示为</strong>： <span class="math display"><em>π</em><sup>*</sup> = arg max<sub><em>π</em></sub><em>J</em>(<em>π</em>)</span></p></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/07/15/hello-blog/" rel="prev" title="Hello Blog">
                  <i class="fa fa-angle-left"></i> Hello Blog
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Liam</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
